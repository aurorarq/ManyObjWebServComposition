Publication
-----------

A. Ramírez, J.A. Parejo, J.R. Romero[*](http://www.jrromero.net), S. Segura and A. Ruiz-Cortés. ["Evolutionary composition of QoS-aware web services: a many-objective perspective"](https://doi.org/10.1016/j.eswa.2016.10.047). _Expert Systems with Applications_, vol. 72, pp. 357-370. 2017.

Abstract
--------

Web service based applications usually invoke services provided by third-parties in their workflow. The Quality of Service (QoS) provided by the invoked supplier can be expressed in terms of the Service Level Agreement specifying the values contracted for particular aspects like cost or throughput, among others. Hence, developers are required to scrutinize the service market in order to select those candidates that best fit with the expected composition focusing on different QoS aspects. This search problem, _a.k.a._ QoS-aware web service composition, is characterized by the presence of many diverse QoS properties to be simultaneously optimized from a multi-objective perspective. This paper explores the suitability of many-objective evolutionary algorithms for tackling the binding problem of web services on the basis of a real-world benchmark with 9 QoS properties. Then, a complete comparative study provides empirical evidence on the adequacy of the most recent and sophisticated techniques to achieve a better trade-off between all the QoS properties. Furthermore, an in-depth study shows that some algorithms are able to promote specific QoS properties while keeping high values for the rest of attributes, enabling appealing advantages for the application of many-objective evolutionary algorithms within the field of service oriented computation.

Highlights
----------

*   QoS-aware web service composition requires multiple simultaneous QoS attributes.
*   Having conflicting QoS properties requires computationally efficient approaches.
*   A comparative experimental study of multi and- many-objective algorithms is presented.
*   Many-objective proposals can promote certain QoS properties while keeping trade-off.

Additional material
-------------------

### Experimental study

*   **Experiment #1**. It considers web service compositions having a maximum of 10, 20, 30, 40, or 50 tasks, where each task contains a different set of candidate services. Combining these elements, a total of 15 problem instances have been generated, i.e. 3 instances per maximum number of tasks, each one associated to a different set of candidate services but sharing the workflow.
*   **Experiment #2**. In order to validate the conclusions drawn from Experiment #1, Experiment #2 should serve to prove that the parameter fixed, i.e the workflow, does not have a marked influence on the outcomes. Therefore, 15 different structures of composition were generated for 3 representative instances, i.e. 10, 30 and 50 tasks, leading to a total of 45 problem instances.

### Problem instances

Problem instances for Experiment #1 and #2 are available in the [data](/data) folder. All the problem instances used in the experimentation were generated by the instance generator proposed in: J.A. Parejo, S. Segura, P. Fernández, A. Ruiz-Cortés. ["QoS-aware web services composition using GRASP with path relinking"](https://doi.org/10.1016/j.eswa.2013.12.036). _Expert Systems with Applications_, vol. 41(9), pp. 4211-4223. The QoS values of the candidate services have been extracted from the [QWS dataset](http://www.uoguelph.ca/~qmahmoud/qws/).

### Experimental results

Results are available for download in Excel format:

*   [Experiment #1](/results/rprsr15-results-experiment1.xlsx)
*   [Experiment #2](/results/rprsr15-results-experiment2.xlsx)

These files contain the mean and standard deviation of the QoS values of the solutions belonging to the Pareto sets returned by each algorithm, as well as the quality indicators used for the statistical validation.

### Statistical tests

#### **Experiment #1**

_Friedman and Holm tests_ Comparison of the algorithms in terms of hypervolume

Friedman test: Iman and Davenport statistic considering reduction performance (distributed according to F-distribution with 7 and 98 degrees of freedom): 63.1879 Critical value at the significance level (alpha=0.01): 2.8272 Holm test: Holm test rejects those hypothesis that have a p-value < 0.025. Comparison of the algorithms in terms of spacing


Friedman test: Iman and Davenport statistic considering reduction performance (distributed according to F-distribution with 7 and 98 degrees of freedom): 202.1765 Critical value at the significance level (alpha=0.01): 2.8272 Holm test: Holm test rejects those hypothesis that have a p-value < 0.025. _Cliff's Delta test (effect size)_ Cliff's Delta test results in raw format:

*   [Hypervolume](/tests/rprsr15-effectsize-experiment1-hv.txt)
*   [Spacing](/tests/rprsr15-effectsize-experiment1-s.txt) 

#### **Experiment #2**

_Friedman and Holm tests_ Comparison of the algorithms in terms of hypervolume


Friedman test: Iman and Davenport statistic considering reduction performance (distributed according to F-distribution with 7 and 308 degrees of freedom): 220.9533 Critical value at the significance level (alpha=0.01): 2.6977 Holm test: Holm test rejects those hypothesis that have a p-value < 0.05. Comparisons of the algorithms in terms of spacing

Friedman test: Iman and Davenport statistic considering reduction performance (distributed according to F-distribution with 7 and 98 degrees of freedom): 453.6330 Critical value at the significance level (alpha=0.01): 2.6977 Holm test: Holm test rejects all the hypotheses. _Cliff's Delta test (effect size)_ Cliff's Delta test results in raw format:

*   [Hypervolume](/tests/rprsr15-effectsize-experiment2-hv.txt)
*   [Spacing](/tests/rprsr15-effectsize-experiment2-s.txt)
